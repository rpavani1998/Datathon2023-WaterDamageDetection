{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgD31GwDdP79","executionInfo":{"status":"ok","timestamp":1683289080185,"user_tz":-120,"elapsed":4406,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}},"outputId":"79f478f3-f101-495f-9799-625eaeafa211"},"id":"MgD31GwDdP79","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pip install dill"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PP7YLZXxhGqh","executionInfo":{"status":"ok","timestamp":1683289116331,"user_tz":-120,"elapsed":6471,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}},"outputId":"666ca961-e495-4374-8198-a8350f59fd42"},"id":"PP7YLZXxhGqh","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.6)\n"]}]},{"cell_type":"code","execution_count":20,"id":"d3bdee4a","metadata":{"id":"d3bdee4a","executionInfo":{"status":"ok","timestamp":1683289361469,"user_tz":-120,"elapsed":805,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import os\n","from PIL import Image\n","from PIL import UnidentifiedImageError\n","import torch\n","from torchvision import datasets, transforms\n","import re\n","import pandas as pd\n","import random\n","import shutil\n","import datetime as dt\n","import torch.nn as nn\n","import torch\n","from pytorch_lightning import LightningModule\n","\n","#LOADING TORCH MODELS\n","import torchvision.models as models\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":5,"id":"80e69004","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683289121088,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"},"user_tz":-120},"id":"80e69004"},"outputs":[],"source":["# Shuffle images into validation and training data \n","# Set the paths for the source folder and the destination folders for the training and validation sets\n","source_folder = r'/content/drive/MyDrive/Bilder'\n","train_folder = r'/content/drive/MyDrive/trainingset'\n","val_folder = r'/content/drive/MyDrive/validationset'\n","augmented_folder=r'/content/drive/MyDrive/Augmented_Image'\n","# Set the validation split percentage (e.g. 20% of the images will be used for validation)\n","val_split = 0.2\n","\n","# Create the training and validation folders if they do not exist\n","os.makedirs(train_folder, exist_ok=True)\n","os.makedirs(val_folder, exist_ok=True)\n","\n","# Get the list of all image files in the source folder\n","image_files = [filename for filename in os.listdir(source_folder) if filename.endswith('.jpg')]\n","\n","augmented_files = [filename for filename in os.listdir(augmented_folder) if filename.endswith('.jpg')]\n","\n","# Shuffle the image files randomly\n","random.shuffle(image_files)\n","\n","# Split the image files into training and validation sets\n","val_size = int(len(image_files) * val_split)\n","train_files = image_files[val_size:] \n","val_files = image_files[:val_size]\n","val_files_id =[re.search(r'ID_(\\d+).*', x).group(1) for x in val_files]\n","\n","# Copy the training images to the training set folder\n","for filename in train_files:\n","    src_path = os.path.join(source_folder, filename)\n","    dst_path = os.path.join(train_folder, filename)\n","    shutil.copy(src_path, dst_path)\n","\n","for filename in augmented_files:\n","    match = re.search(r'ID_(\\d+).*', filename)\n","    image_id = match.group(1)\n","    if(image_id in val_files_id):\n","        continue\n","    src_path = os.path.join(augmented_folder, filename)\n","    dst_path = os.path.join(train_folder, filename)\n","    shutil.copy(src_path, dst_path)\n","\n","\n","# Copy the validation images to the validation set folder\n","for filename in val_files:\n","    src_path = os.path.join(source_folder, filename)\n","    dst_path = os.path.join(val_folder, filename)\n","    shutil.copy(src_path, dst_path)"]},{"cell_type":"code","execution_count":23,"id":"c401406a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"elapsed":1054,"status":"ok","timestamp":1683289427120,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"},"user_tz":-120},"id":"c401406a","outputId":"ec7b2b04-71cb-4d2a-9333-946b5b8e25ee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ID  Etage ganze Zahl  Dauer des Schadens in Tagen  Label  Material:Beton  \\\n","0   1                 3                            4      4               1   \n","1   2                 0                           10      2               0   \n","2   3                 1                            5      0               1   \n","3   4                 3                            7      6               1   \n","\n","   Material:Holz  Material:Stein  Material:Styropor  Age  \n","0              0               0                  0   56  \n","1              0               1                  0   22  \n","2              0               0                  0  107  \n","3              0               0                  0   59  "],"text/html":["\n","  <div id=\"df-eaa3a9c0-3a91-45b9-ba0e-fb6949891227\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Etage ganze Zahl</th>\n","      <th>Dauer des Schadens in Tagen</th>\n","      <th>Label</th>\n","      <th>Material:Beton</th>\n","      <th>Material:Holz</th>\n","      <th>Material:Stein</th>\n","      <th>Material:Styropor</th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>107</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>59</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eaa3a9c0-3a91-45b9-ba0e-fb6949891227')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-eaa3a9c0-3a91-45b9-ba0e-fb6949891227 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-eaa3a9c0-3a91-45b9-ba0e-fb6949891227');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":23}],"source":["#loading datasets\n","excel_data = pd.read_excel(r'/content/drive/MyDrive/WD_Features_and_Costs.xlsx')\n","excel_data['Label'] = pd.cut(excel_data['Schadenhöhe'], [x for x in range(0,60001,10000)]+[90000],labels=[0,1,2,3,4,5,6])\n","excel_data['Label'] = excel_data['Label'].astype(int)\n","\n","excel_data = excel_data[['ID', 'Material', 'Etage ganze Zahl', 'Baujahr','Dauer des Schadens in Tagen', 'Postleitzahl', 'Schadenhöhe','Label']]\n","\n","Dummy_Variables=[pd.get_dummies(excel_data['Material'],prefix = 'Material', prefix_sep = ':')]\n","Dummy_Variables=pd.concat(Dummy_Variables,axis=1)\n","excel_data=pd.concat([excel_data,Dummy_Variables],axis=1)\n","excel_data['Material:Styropor'] = 0\n","excel_data[\"Age\"]=int(dt.datetime.today().strftime(\"%Y\"))-excel_data[\"Baujahr\"]\n","\n","excel_data = excel_data.drop(columns=[\"Material\",\"Postleitzahl\",\"Baujahr\",\"Schadenhöhe\"])\n","\n","#Displaying first 4 details\n","excel_data[:4]"]},{"cell_type":"markdown","source":["## Custom Dataset"],"metadata":{"id":"PWnFRbgQ3rFJ"},"id":"PWnFRbgQ3rFJ"},{"cell_type":"code","execution_count":24,"id":"c88c2b87","metadata":{"id":"c88c2b87","executionInfo":{"status":"ok","timestamp":1683289493812,"user_tz":-120,"elapsed":983,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","        self.indices = list(range(len(data)))\n","\n","    def __getitem__(self, index):\n","        # return both the data and the index\n","        return self.data[index], self.indices[index]\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":25,"id":"dabdc441","metadata":{"id":"dabdc441","executionInfo":{"status":"ok","timestamp":1683289494479,"user_tz":-120,"elapsed":3,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["def formTensorData(directory, reshape_size):\n","    \n","\n","    convert_tensor = transforms.ToTensor()\n","\n","    #resize every image to input shape\n","    input_shape = (reshape_size, reshape_size)\n","    files = os.listdir(directory)\n","    inputImgs = []\n","    inputImgs_labels = []\n","    features = []\n","    for filename in files:\n","        \n","        match = re.search(r'ID_(\\d+).*', filename)\n","        image_id = int(match.group(1))\n","        if filename.endswith('.jpg'):\n","            try:\n","\n","                file_path = os.path.join(directory, filename)\n","                img = Image.open(file_path).convert('RGB')\n","                \n","                if os.path.getsize(file_path) > 0:\n","                    img = img.resize(input_shape[:2])\n","                    X = convert_tensor(img)\n","                    X_label = excel_data[excel_data['ID'] == image_id].iloc[0,:]['Label']\n","\n","                    #preparing output\n","                    inputImgs_labels.append(X_label)\n","                    excel_features = excel_data[excel_data['ID'] == image_id].drop(columns=['Label','ID']).iloc[0,:]\n","                    inputImgs.append(X)\n","                    features.append(torch.Tensor(list(excel_features.values)))\n","                    \n","            except UnidentifiedImageError:\n","                print(f\"{filename} is not a valid image file\")\n","            \n","                \n","    return torch.stack(inputImgs), inputImgs_labels, features"]},{"cell_type":"code","execution_count":26,"id":"e330990a","metadata":{"executionInfo":{"elapsed":20503,"status":"ok","timestamp":1683289519060,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"},"user_tz":-120},"id":"e330990a"},"outputs":[],"source":["X, X_label, features = formTensorData(train_folder,224)\n","X_valid, X_valid_label, val_features = formTensorData(val_folder,224)"]},{"cell_type":"code","execution_count":27,"id":"1eefddc6","metadata":{"id":"1eefddc6","executionInfo":{"status":"ok","timestamp":1683289519062,"user_tz":-120,"elapsed":9,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["X=X.to(DEVICE)\n","X_valid=X_valid.to(DEVICE)"]},{"cell_type":"code","execution_count":28,"id":"7976d269","metadata":{"id":"7976d269","executionInfo":{"status":"ok","timestamp":1683289519063,"user_tz":-120,"elapsed":7,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["X_label = torch.Tensor(X_label).type(torch.LongTensor).to(DEVICE)\n","features = torch.stack(features).to(DEVICE)\n","X_valid_label = torch.Tensor(X_valid_label).type(torch.LongTensor).to(DEVICE)\n","X_val_features = torch.stack(val_features).to(DEVICE)"]},{"cell_type":"markdown","source":["## TRAIN & TEST : ADAM OPTIMIZER"],"metadata":{"id":"kEtvhH1WuVQj"},"id":"kEtvhH1WuVQj"},{"cell_type":"code","execution_count":31,"id":"0df89ef7","metadata":{"id":"0df89ef7","executionInfo":{"status":"ok","timestamp":1683289749177,"user_tz":-120,"elapsed":582,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["# train model\n","from tqdm import tqdm\n","\n","def m_train(\n","    model, num_epochs=5, learning_rate=0.001, batch_size=10, reshape_1d=False\n","):\n","    \n","    my_dataset = MyDataset(X)\n","\n","    # create a DataLoader\n","    valid_loader = DataLoader(my_dataset, batch_size=batch_size)\n","    \n","    #Instantiating loss function\n","    loss_fn = nn.NLLLoss()\n","    \n","    #Instantiating optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    \n","    for e in tqdm(range(num_epochs)):\n","        for batch, indices in valid_loader:\n","            inputs = batch\n","            labels = X_label[indices]\n","            batch_features = features[indices]\n","            forward = model(inputs,batch_features)\n","            loss = loss_fn(forward, labels)\n","            print(loss)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        \n","    return model"]},{"cell_type":"code","execution_count":32,"id":"c7d847e1","metadata":{"id":"c7d847e1","executionInfo":{"status":"ok","timestamp":1683289750278,"user_tz":-120,"elapsed":3,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["# test model\n","@torch.no_grad()\n","def m_test(model, batch_size=5, reshape_1d=False):\n","    \"\"\"\n","    Function to test your CNN on validation data\n","\n","    Parameters\n","    ----------\n","\n","    model: trained CNN from task 1a\n","    batch_size: size of batch for dataloader\n","    reshape_1d: Reshape images to a 1d vectors (allows use of models other than CNNs\n","                such as fully-connected FNNs)\n","\n","    Returns\n","    -------\n","    accuracy of input model\n","    \"\"\"\n","    correct = 0  # number of correct predictions\n","    total = 0  # total number of examples\n","    model.eval()  # set layers like dropout and batch norm to eval mode\n","    total_predictions = torch.tensor([]).to(DEVICE)\n","\n","    # Create validation data loader\n","    \n","    my_dataset = MyDataset(X_valid)\n","\n","    # create a DataLoader\n","    valid_loader = DataLoader(my_dataset, batch_size=batch_size)\n","    \n","    # Loop over data\n","    for batch,indices in valid_loader:\n","        indices=indices.to(DEVICE)\n","        inputs = batch\n","        labels = X_valid_label[indices]\n","        batch_features = X_val_features[indices]\n","        print(indices)\n","        print(labels)\n","\n","        total += inputs.size(dim=0)\n","        \n","        out = model(inputs,batch_features)\n","        _, predictions = torch.max(out, dim = 1)\n","        print(predictions)\n","        \n","        total_predictions = torch.cat((total_predictions,predictions),dim=0)\n","        correct+= torch.sum(predictions == labels).item()\n","        pass\n","\n","    accuracy = (correct / total) * 100\n","    print(\"Accuracy on {} validation images: {} %\".format(total, accuracy))\n","    return accuracy,total_predictions"]},{"cell_type":"markdown","source":["## SGD OPTIMIZER (less accuracy)"],"metadata":{"id":"YBuH0uIrusXh"},"id":"YBuH0uIrusXh"},{"cell_type":"code","source":["# # train model\n","# from tqdm import tqdm\n","\n","# def mnist_train(\n","#     model, num_epochs=5, learning_rate=0.001, batch_size=10, reshape_1d=False\n","# ):\n","    \n","#     my_dataset = MyDataset(X)\n","\n","#     # create a DataLoader\n","#     valid_loader = DataLoader(my_dataset, batch_size=batch_size)\n","    \n","#     #Instantiating loss function\n","#     loss_fn = nn.NLLLoss()\n","    \n","#     #Instantiating optimizer\n","#     # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","#     SGD_optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","    \n","#     for e in tqdm(range(num_epochs)):\n","#         for batch, indices in valid_loader:\n","#             inputs = batch\n","#             labels = X_label[indices]\n","#             batch_features = features[indices]\n","#             forward = model(inputs,batch_features)\n","#             loss = loss_fn(forward, labels)\n","#             print(loss)\n","#             SGD_optimizer.zero_grad()\n","#             loss.backward()\n","#             SGD_optimizer.step()\n","        \n","#     return model"],"metadata":{"id":"ZzC1-9jhuTmb","executionInfo":{"status":"ok","timestamp":1683289753180,"user_tz":-120,"elapsed":2,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"id":"ZzC1-9jhuTmb","execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## CNN Pretrained"],"metadata":{"id":"LREm-QP9M28c"},"id":"LREm-QP9M28c"},{"cell_type":"code","execution_count":34,"id":"44b24364","metadata":{"id":"44b24364","executionInfo":{"status":"ok","timestamp":1683289754860,"user_tz":-120,"elapsed":5,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"outputs":[],"source":["class CNN_Pretrained(nn.Module):\n","    def __init__(self):\n","        super(CNN_Pretrained, self).__init__()\n","        self.sequential = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","        )\n","\n","        self.backbone = models.resnet50(pretrained=True)\n","        \n","        #freeze pre-trained layer\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","                            \n","        self.linear1 = nn.Linear(1000, 249)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.linear2 = nn.Linear(249+7, 128)\n","        self.linear3 = nn.Linear(128,7)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x, features):\n","        out = None\n","        out = self.sequential(x)\n","        out = self.backbone(out)\n","        out = self.linear1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = torch.cat((out,features),dim = 1)\n","        out = self.linear2(out)\n","        out = self.linear3(out)\n","        out = self.softmax(out)\n","        \n","        return out"]},{"cell_type":"code","source":["# Train a model.\n","model_with_pretrain = m_train(CNN_Pretrained().to(DEVICE), batch_size=40, num_epochs = 10)\n","torch.save(model_with_pretrain, \"/content/drive/MyDrive/cnn_pretrained.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmQRFQB0sLuu","executionInfo":{"status":"ok","timestamp":1683289806324,"user_tz":-120,"elapsed":50345,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}},"outputId":"a78e8309-9d28-424f-ab9a-beba374f5571"},"id":"fmQRFQB0sLuu","execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","  0%|          | 0/10 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["tensor(2.8404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.3457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.9260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5825, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 1/10 [00:04<00:43,  4.89s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.4435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.2820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.2618, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [00:10<00:40,  5.03s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.2589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.2060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8973, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [00:15<00:35,  5.09s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.0224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [00:20<00:30,  5.10s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [00:25<00:25,  5.09s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.5391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [00:30<00:20,  5.06s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [00:35<00:15,  5.02s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [00:40<00:09,  4.99s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [00:45<00:04,  4.96s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:49<00:00,  5.00s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(0.5015, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["accuracy,predictions = m_test(model_with_pretrain)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPXeaGMPsKrZ","executionInfo":{"status":"ok","timestamp":1683289833125,"user_tz":-120,"elapsed":1001,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}},"outputId":"136c07c6-e746-4a51-cefc-9c446f47796f"},"id":"yPXeaGMPsKrZ","execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4], device='cuda:0')\n","tensor([6, 0, 1, 1, 4], device='cuda:0')\n","tensor([6, 0, 0, 5, 0], device='cuda:0')\n","tensor([5, 6, 7, 8, 9], device='cuda:0')\n","tensor([3, 4, 2, 1, 5], device='cuda:0')\n","tensor([3, 3, 2, 4, 1], device='cuda:0')\n","tensor([10, 11, 12, 13, 14], device='cuda:0')\n","tensor([2, 0, 1, 1, 0], device='cuda:0')\n","tensor([1, 3, 4, 4, 0], device='cuda:0')\n","tensor([15, 16, 17, 18, 19], device='cuda:0')\n","tensor([4, 4, 0, 5, 0], device='cuda:0')\n","tensor([1, 4, 3, 0, 1], device='cuda:0')\n","tensor([20, 21, 22, 23, 24], device='cuda:0')\n","tensor([2, 0, 1, 3, 1], device='cuda:0')\n","tensor([4, 3, 3, 1, 1], device='cuda:0')\n","tensor([25, 26, 27, 28], device='cuda:0')\n","tensor([4, 2, 6, 1], device='cuda:0')\n","tensor([4, 2, 3, 1], device='cuda:0')\n","Accuracy on 29 validation images: 34.48275862068966 %\n"]}]},{"cell_type":"markdown","source":["# Concatenation UNet Images and CNN network"],"metadata":{"id":"mMmJdNO-7zaI"},"id":"mMmJdNO-7zaI"},{"cell_type":"code","source":["import torch\n","import pytorch_lightning as pl\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchmetrics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import dill as pickle\n","import torchvision.transforms.functional as TF\n","import os\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.transforms.functional as Func\n","\n","class Block(pl.LightningModule):\n","    \n","    def _init_(self, in_channels, out_channels, kernel_size=3):\n","        super()._init_()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Encoder(pl.LightningModule):\n","\n","    def _init_(self, in_channels, out_channels):\n","        super()._init_()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            Block(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","class Decoder(pl.LightningModule):\n","\n","    def _init_(self, in_channels, out_channels):\n","        super()._init_()\n","        self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = Block(in_channels, out_channels)\n","\n","    def forward(self, expand_input, contract_input):\n","        expand_output = self.up(expand_input)\n","        if expand_output.shape != contract_input.shape:\n","                expand_output = TF.resize(expand_output, size=contract_input.shape[2:])\n","        concat_output = torch.cat((expand_output, contract_input), dim=1)\n","        return self.conv(concat_output)\n","\n","class UNet(pl.LightningModule):\n","\n","    def _init_(self, chn=(3,64,128,256,512, 1024)):\n","        super()._init_()\n","        self.learning_rate = 1e-3 #1e-3\n","\n","        self.train_pixel_acc = torchmetrics.classification.BinaryAccuracy(num_classes=2, ignore= 255, average='weighted')\n","        self.train_J = torchmetrics.classification.BinaryJaccardIndex(num_classes=2, ignore= 255, average='weighted')\n","\n","        self.val_pixel_acc = torchmetrics.classification.BinaryAccuracy(num_classes=2, ignore= 255 , average='weighted')\n","        self.val_J = torchmetrics.classification.BinaryJaccardIndex(num_classes=2, ignore= 255 , average='weighted')\n","\n","        self.test_pixel_acc = torchmetrics.classification.BinaryAccuracy(num_classes=2, ignore= 255, average='weighted')\n","        self.test_J = torchmetrics.classification.BinaryJaccardIndex(num_classes=2, ignore= 255, average='weighted')      \n","        # encoder\n","        self.first_layer = Block(chn[0], chn[1])\n","        self.encode_layers = nn.ModuleList()\n","        self.decode_layers = nn.ModuleList()\n","        \n","        for i in range(1, len(chn)-2):\n","            self.encode_layers.append(Encoder(chn[i],chn[i+1]))\n","            \n","        # bottleneck\n","        self.bottleneck = Block(chn[-2], chn[-1])\n","        # decoder\n","                    \n","        for i in range(len(chn)-1, 1, -1):\n","            self.decode_layers.append(Decoder(chn[i], chn[i-1]))\n","            \n","        self.final_layer = nn.Conv2d(chn[1], 1, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        \n","        \n","        #############################\n","\n","    def forward(self, x):\n","        # TODO: Define U-Net layers #\n","        x = self.first_layer(x)\n","        connections = []\n","        connections.append(x)\n","        for encode_layer in self.encode_layers:\n","            x = encode_layer(x)\n","            connections.append(x)\n","            \n","        x = self.bottleneck(x)\n","        connections = connections[::-1]\n","\n","        for i in range(len(self.decode_layers)):\n","            x = self.decode_layers[i](x, connections[i])\n","                           \n","        return self.sigmoid(self.final_layer(x))\n","        #############################\n","\n","    def decode_segmap(self, prediction):\n","        label_colors = torch.tensor([(0, 64, 128)])\n","        r = torch.zeros_like(prediction, dtype=torch.uint8)\n","        g = torch.zeros_like(prediction, dtype=torch.uint8)\n","        b = torch.zeros_like(prediction, dtype=torch.uint8)\n","        for l in range(0, self.num_class):\n","            idx = prediction == l\n","            r[idx] = label_colors[l, 0]\n","            g[idx] = label_colors[l, 1]\n","            b[idx] = label_colors[l, 2]\n","        rgb = torch.stack([r, g, b], axis=1)\n","        return rgb\n","\n","    def training_step(self, train_batch, batch_idx):\n","        images, seg_mask = train_batch\n","\n","        # convert the non-binary seg_mask to binary\n","        seg_mask_binary = torch.where(seg_mask > 0, torch.tensor(1), torch.tensor(0))\n","        seg_mask_binary = seg_mask_binary.squeeze(1).to(device)\n","        outputs = self(images)\n","        pred_seg_mask = torch.argmax(outputs, 1).to(device)\n","        outputs = outputs.squeeze(1).to(device)\n","        pred_seg_mask = pred_seg_mask.squeeze(1).to(device)\n","        # pixel-wise accuracy\n","        self.train_pixel_acc(pred_seg_mask, seg_mask_binary)\n","\n","        # the Jaccard index (mean IoU)\n","        self.train_J(pred_seg_mask, seg_mask_binary)\n","        \n","        self.log('train_acc', self.train_pixel_acc, on_step=False, on_epoch=True)\n","        self.log('train_mIoU', self.train_J, on_step=False, on_epoch=True)\n","\n","        # convert seg_mask_binary to float\n","        seg_mask_binary = seg_mask_binary.float()\n","\n","        # loss\n","        loss = F.binary_cross_entropy(outputs.squeeze(1).to(device), seg_mask_binary.float())\n","        self.log('train_loss', loss, on_step=False, on_epoch=True)\n","        print(\"Train :\", loss, self.train_pixel_acc)\n","        return loss\n","\n","\n","\n","    def validation_step(self, val_batch, batch_idx):\n","      images, seg_mask = val_batch\n","\n","      seg_mask_binary = torch.where(seg_mask > 0, torch.tensor([1]).to(device), torch.tensor([0]).to(device))\n","      seg_mask_binary = seg_mask_binary.squeeze(1).to(device)\n","      outputs = self(images)\n","      pred_seg_mask = torch.argmax(outputs, 1)\n","      outputs = outputs.squeeze(1).to(device)\n","      pred_seg_mask = pred_seg_mask.squeeze(1).to(device)\n","      print(outputs.shape, pred_seg_mask.shape, seg_mask_binary.shape)\n","      # pixel-wise accuracy\n","      self.val_pixel_acc(pred_seg_mask, seg_mask_binary)\n","\n","      # the Jaccard index (mean IoU)\n","      self.val_J(pred_seg_mask, seg_mask_binary)\n","\n","      self.log('val_pixel_acc', self.val_pixel_acc, on_step=False, on_epoch=True)\n","      # self.log('val_mIoU', self.val_J, on_step=False, on_epoch=True)\n","      print(\"Valid:\",  self.val_pixel_acc, self.val_J)\n","      loss = F.binary_cross_entropy(outputs.squeeze(1).to(device), seg_mask_binary.float())\n","      self.log('valid_loss', loss, on_step=False, on_epoch=True)\n","      return loss\n","\n","\n","\n","    def test_step(self, batch, batch_idx):\n","      images, seg_mask = batch\n","\n","      seg_mask_binary = torch.where(seg_mask > 0, torch.tensor([1]).to(device), torch.tensor([0]).to(device))\n","      seg_mask_binary = seg_mask_binary.squeeze(1).to(device)\n","\n","      outputs = self(images)\n","      pred_seg_mask = torch.argmax(outputs, 1)\n","      outputs = outputs.squeeze(1).to(device)\n","      pred_seg_mask = pred_seg_mask.squeeze(1).to(device)\n","      print(outputs.shape, pred_seg_mask.shape, seg_mask_binary.shape)\n","      self.test_pixel_acc(pred_seg_mask, seg_mask_binary)\n","      self.test_J(pred_seg_mask, seg_mask_binary)\n","      print(\"Test:\",  self.test_pixel_acc, self.val_J)\n","      self.log(\"test_acc\", self.test_pixel_acc)\n","      # self.log(\"test_mIoU\", self.test_J)\n","\n","      loss = F.binary_cross_entropy(outputs.squeeze(1), seg_mask_binary.float())\n","      self.log('test_loss', loss, on_step=False, on_epoch=True)\n","      return loss\n","        \n","\n","    def configure_optimizers(self):\n","        opt = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n","        return opt"],"metadata":{"id":"0_gWGYPwhAFx","executionInfo":{"status":"ok","timestamp":1683289929997,"user_tz":-120,"elapsed":493,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"id":"0_gWGYPwhAFx","execution_count":37,"outputs":[]},{"cell_type":"code","source":["# load the checkpoint\n","unet_model = torch.load('/content/drive/MyDrive/unet_water_damage.pt')\n","unet_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jyYT4_E8ORP","executionInfo":{"status":"ok","timestamp":1683289932473,"user_tz":-120,"elapsed":629,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}},"outputId":"2819c1da-2106-4670-be6c-29aa6547812f"},"id":"9jyYT4_E8ORP","execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["UNet(\n","  (train_pixel_acc): BinaryAccuracy()\n","  (train_J): BinaryJaccardIndex()\n","  (val_pixel_acc): BinaryAccuracy()\n","  (val_J): BinaryJaccardIndex()\n","  (test_pixel_acc): BinaryAccuracy()\n","  (test_J): BinaryJaccardIndex()\n","  (first_layer): Block(\n","    (conv): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","    )\n","  )\n","  (encode_layers): ModuleList(\n","    (0): Encoder(\n","      (maxpool_conv): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (1): Block(\n","          (conv): Sequential(\n","            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (5): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (1): Encoder(\n","      (maxpool_conv): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (1): Block(\n","          (conv): Sequential(\n","            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (5): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","    (2): Encoder(\n","      (maxpool_conv): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (1): Block(\n","          (conv): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (5): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (decode_layers): ModuleList(\n","    (0): Decoder(\n","      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n","      (conv): Block(\n","        (conv): Sequential(\n","          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (5): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (1): Decoder(\n","      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (conv): Block(\n","        (conv): Sequential(\n","          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (5): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (2): Decoder(\n","      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n","      (conv): Block(\n","        (conv): Sequential(\n","          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (5): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (3): Decoder(\n","      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n","      (conv): Block(\n","        (conv): Sequential(\n","          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (5): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","  )\n","  (bottleneck): Block(\n","    (conv): Sequential(\n","      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","    )\n","  )\n","  (final_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["class CNN_Pretrained_UNet(nn.Module):\n","    def __init__(self):\n","        super(CNN_Pretrained_UNet, self).__init__()\n","        self.unet = unet_model\n","         #freeze pre-trained layer\n","        for param in self.unet.parameters():\n","            param.requires_grad = False\n","\n","        self.sequential = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","        )\n","        self.unet_sequential = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(4, stride = 4),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU())\n","\n","        self.backbone = models.resnet50(pretrained=True)\n","        \n","        #freeze pre-trained layer\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","                            \n","        self.linearUnet = nn.Linear(56*56*16,1000)\n","        self.linear1 = nn.Linear(2000, 249)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.linear2 = nn.Linear(249+7, 128)\n","        self.linear3 = nn.Linear(128,7)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x, features):\n","        out = None\n","        out = self.sequential(x)\n","        out = self.backbone(out)\n","        out1 = self.unet(x)\n","        out1 = self.unet_sequential(out1)\n","        out1 = self.linearUnet(out1.view(-1,(56*56*16)))\n","        out = torch.cat((out,out1),dim = 1)\n","        out = self.linear1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = torch.cat((out,features),dim = 1)\n","        out = self.linear2(out)\n","        out = self.linear3(out)\n","        out = self.softmax(out)\n","        \n","        return out"],"metadata":{"id":"J1fpI68X7Zv1","executionInfo":{"status":"ok","timestamp":1683289933448,"user_tz":-120,"elapsed":3,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"}}},"id":"J1fpI68X7Zv1","execution_count":39,"outputs":[]},{"cell_type":"code","execution_count":40,"id":"da967123","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133704,"status":"ok","timestamp":1683290075621,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"},"user_tz":-120},"id":"da967123","outputId":"3df0eb3d-5f03-46a5-b8c4-8c44b58391c1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["tensor(3.0106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(13.3048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(12.1557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(11.0361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(7.9152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(6.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(3.9929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(4.1859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(3.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(3.8035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.8435, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 1/10 [00:12<01:55, 12.84s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(3.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.1381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.1852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(3.4541, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.7782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(2.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.9804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [00:26<01:46, 13.26s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.8462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.9614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7397, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7642, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [00:39<01:33, 13.37s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5087, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [00:52<01:19, 13.26s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5867, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [01:05<01:05, 13.11s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.5955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.2984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.8242, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4228, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [01:18<00:52, 13.02s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.4942, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6255, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.7748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3896, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [01:31<00:38, 12.97s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.2448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4886, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [01:44<00:25, 12.99s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4975, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.6198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3945, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [01:57<00:13, 13.03s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.5449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.3992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4478, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.4150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.5002, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [02:10<00:00, 13.08s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor(1.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Train a model.\n","model_with_pretrain_unet = m_train(CNN_Pretrained_UNet().to(DEVICE), batch_size=40, num_epochs = 10)\n","torch.save(model_with_pretrain_unet, \"/content/drive/MyDrive/cnn_pretrained_unet.pt\")"]},{"cell_type":"code","execution_count":41,"id":"c3e6d62a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1276,"status":"ok","timestamp":1683290076883,"user":{"displayName":"Aakriti Istwal","userId":"08898898745772089984"},"user_tz":-120},"id":"c3e6d62a","outputId":"8807c18e-8806-471c-8d88-9ff257c6fe56"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4], device='cuda:0')\n","tensor([6, 0, 1, 1, 4], device='cuda:0')\n","tensor([1, 1, 3, 1, 3], device='cuda:0')\n","tensor([5, 6, 7, 8, 9], device='cuda:0')\n","tensor([3, 4, 2, 1, 5], device='cuda:0')\n","tensor([3, 4, 2, 2, 1], device='cuda:0')\n","tensor([10, 11, 12, 13, 14], device='cuda:0')\n","tensor([2, 0, 1, 1, 0], device='cuda:0')\n","tensor([2, 3, 4, 1, 1], device='cuda:0')\n","tensor([15, 16, 17, 18, 19], device='cuda:0')\n","tensor([4, 4, 0, 5, 0], device='cuda:0')\n","tensor([4, 2, 0, 1, 3], device='cuda:0')\n","tensor([20, 21, 22, 23, 24], device='cuda:0')\n","tensor([2, 0, 1, 3, 1], device='cuda:0')\n","tensor([1, 3, 3, 4, 1], device='cuda:0')\n","tensor([25, 26, 27, 28], device='cuda:0')\n","tensor([4, 2, 6, 1], device='cuda:0')\n","tensor([4, 0, 3, 4], device='cuda:0')\n","Accuracy on 29 validation images: 34.48275862068966 %\n"]}],"source":["accuracy,predictions = m_test(model_with_pretrain_unet)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1h62j1n82q_Uy0Wfnr9TuR6lqMQjGHsKa","timestamp":1683269538355}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}